{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cross over conversation generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G898tNB-rGU"
      },
      "source": [
        "# Cross over conversation generator\n",
        "This note book only works on google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YHZjlBtU005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b753d45-1963-4b2e-e57b-c89cd6fbea70"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OxwXgnjLyOk"
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import random\n",
        "import re\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Nwti2ZLyQd"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9K1v73J9X86"
      },
      "source": [
        "# Introduction of the checkpoints\n",
        "There are four models available for generating in this notebook.Before generating anything, all of the check point files need to be uploaded into a folder called \"checkpoint\", and the folder \"checkpoint\" needs to be created under \"content\" folder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBTBj4xOLySU"
      },
      "source": [
        "sess1 = gpt2.start_tf_sess()\n",
        "sess2 = gpt2.start_tf_sess()\n",
        "sess3 = gpt2.start_tf_sess()\n",
        "sess4 = gpt2.start_tf_sess()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWIcSlWaLyUG",
        "outputId": "44503e42-8ee5-410b-ddc3-3075ae892ed9"
      },
      "source": [
        "gpt2.load_gpt2(sess1, run_name='Bilbo')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/Bilbo/model-100\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/Bilbo/model-100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07Dq9iX-LyWL",
        "outputId": "968d564b-1633-499c-9f97-76aa568f03bb"
      },
      "source": [
        "tf.get_variable_scope().reuse_variables()\n",
        "gpt2.load_gpt2(sess2, run_name='Hagrid')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/Hagrid/model-100\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/Hagrid/model-100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UaW0now4XoX",
        "outputId": "e7517183-9505-47de-ce40-8bc51003847e"
      },
      "source": [
        "tf.get_variable_scope().reuse_variables()\n",
        "gpt2.load_gpt2(sess3, run_name='Gandalf')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/Gandalf/model-100\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/Gandalf/model-100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Mj71lai4Xso",
        "outputId": "2d38ae6f-6681-466c-9a51-f1faedfcc170"
      },
      "source": [
        "tf.get_variable_scope().reuse_variables()\n",
        "gpt2.load_gpt2(sess4, run_name='Dumbledore')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/Dumbledore/model-100\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/Dumbledore/model-100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWGkWzc_7NG3"
      },
      "source": [
        "# Introduction of \n",
        "# function generate_2(model_name1, model_name2, text, max_number)\n",
        "To make it easier to generate a conversation, I wrote the following function，the idea is to let two models generate prefix for each other. To mantain the consistency within each conversation, i made some effort to unify the format of each generated sentence, in order to avoid generating empty spaces or single punctuation marks.However, my method can not completely eliminate the generation of empty information, but can make the probability of such events relatively smaller.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmnwOrqzWB3U"
      },
      "source": [
        "\n",
        "def generate_2(model_name1, model_name2, text, max_number):\n",
        "  text = str(text)\n",
        "  session = {'Bilbo':sess1, 'Hagrid':sess2, 'Gandalf':sess3, 'Dumbledore':sess4}\n",
        "  out = {'Bilbo':'[Bilbo out]', 'Hagrid':'[Hagrid out]', 'Gandalf':'[out]', 'Dumbledore':'[out]'}\n",
        "  max_number = int(max_number)\n",
        "  conversation = []\n",
        "  pattern = re.compile(r'\\W+$')\n",
        "\n",
        "  \n",
        "  while len(conversation)<=max_number:\n",
        "    if len(conversation)<=0:\n",
        "        text = gpt2.generate(session[model_name1], \n",
        "                             return_as_list=True, \n",
        "                             prefix=text, \n",
        "                             nsamples=1, \n",
        "                             length=30, temperature=0.5, \n",
        "                             run_name=model_name1, \n",
        "                             truncate =out[model_name1], \n",
        "                             include_prefix = True,\n",
        "                             )\n",
        "        text = str(text[0])\n",
        "        text = re.sub(pattern,',',text)\n",
        "        list = ['0']\n",
        "        list[0] = text\n",
        "        conversation.append(list)\n",
        "\n",
        "    text = gpt2.generate(session[model_name2], \n",
        "                     return_as_list=True, \n",
        "                     prefix=list[0], \n",
        "                     nsamples=1, \n",
        "                     length=30, temperature=0.5, \n",
        "                     run_name=model_name2, \n",
        "                     truncate = out[model_name2], \n",
        "                     include_prefix = False,\n",
        "                     )\n",
        "    text = str(text[0])\n",
        "    text = re.sub(pattern,',',text)\n",
        "    list = ['0']\n",
        "    list[0] = text\n",
        "    conversation.append(list)\n",
        "\n",
        "    text = gpt2.generate(session[model_name1], \n",
        "                     return_as_list=True, \n",
        "                     prefix=list[0], \n",
        "                     nsamples=1, \n",
        "                     length=30, temperature=0.5, \n",
        "                     run_name=model_name1, \n",
        "                     truncate = out[model_name1], \n",
        "                     include_prefix = False,\n",
        "                     )\n",
        "    text = str(text[0])\n",
        "    text = re.sub(pattern,',',text)\n",
        "    list = ['0']\n",
        "    list[0] = text\n",
        "    conversation.append(list) \n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "  conversation_new = []\n",
        "  for i in range(len(conversation)):\n",
        "    line = conversation[i]\n",
        "    if line == ['']:\n",
        "      line = '[......]'\n",
        "    conversation_new.append(line)\n",
        "  for i in range(len(conversation_new)):\n",
        "    if i%2 == 0:\n",
        "      print('[', model_name1, ':]', conversation_new[i])\n",
        "    elif i%2 != 0:\n",
        "      print('[', model_name2, ':]', conversation_new[i])\n",
        "\n",
        "  \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BErgj8X6drwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b6134d-10d7-4687-e273-9bedd9a89a6c"
      },
      "source": [
        "generate_2('Bilbo', 'Hagrid', \"I think dragons are\", 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Bilbo :] ['I think dragons are a terrible lot and the worst thing they can do is cause trouble,']\n",
            "[ Hagrid :] [\" even if it's just for show,\"]\n",
            "[ Bilbo :] [' you know,']\n",
            "[ Hagrid :] [\" I think he killed 'im one day\"]\n",
            "[ Bilbo :] [',']\n",
            "[ Hagrid :] [\"Hagrid out]Tha's the one\"]\n",
            "[ Bilbo :] [' thing I never forgave you, oh Nagrand’s Smaug the Stupendous,']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBsIoIWtbCnA",
        "outputId": "75ac46c4-261c-47b7-cf5d-0d81c3b74302"
      },
      "source": [
        "generate_2('Hagrid','Bilbo', 'I think dragons are', 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Hagrid :] ['I think dragons are gonna like me']\n",
            "[ Bilbo :] [' for it,']\n",
            "[ Hagrid :] [\" but I don't think so,\"]\n",
            "[ Bilbo :] [' either,']\n",
            "[ Hagrid :] [\" but I reckon he'll be glad of it,\"]\n",
            "[ Bilbo :] [' for his guard is up,']\n",
            "[ Hagrid :] [\" but I'm not there yet,\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ_4t-qnE7iU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee5ef2b-51c1-4b4b-aba7-51bf64bf65b7"
      },
      "source": [
        "generate_2('Gandalf', 'Dumbledore', \"Good morning\", 5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Gandalf :] ['Good morning, all.\" ―Fili, Dain, Balin, Bifur, Bofur, Bofur-ul-Fili,']\n",
            "[ Dumbledore :] [\" 'The Hall goes dark,\"]\n",
            "[ Gandalf :] [' and the only thing left to do is leave the books in the room,']\n",
            "[ Dumbledore :] [' so Sirius can return them,']\n",
            "[ Gandalf :] [' if he so chooses,']\n",
            "[ Dumbledore :] [\" but I'm afraid he'll find it much more challenging than I imagined,\"]\n",
            "[ Gandalf :] [' and he does than which I can only speak Persian,']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bDehiNf5Fad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61775f32-cf0d-4316-9838-7121bf740736"
      },
      "source": [
        "generate_2('Dumbledore','Gandalf', \"Good morning\", 5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Dumbledore :] ['Good morning, Harry,']\n",
            "[ Gandalf :] [' and good morning to our friends,']\n",
            "[ Dumbledore :] [' Minerva,']\n",
            "[ Gandalf :] [' or else it will fall into the wrong hands,']\n",
            "[ Dumbledore :] [' allowing the wrong person to take it,']\n",
            "[ Gandalf :] [' and thus the game of golf is abandoned,']\n",
            "[ Dumbledore :] [' the gamekeeper is more than willing to send the gamekeeper to the dungeons']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M0T18N850co"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}